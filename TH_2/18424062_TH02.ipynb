{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "18424062_TH02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XFEzNQBb2wHF"
      },
      "source": [
        "# Thực hành 2 : Cách thực thi song song trên CUDA\n",
        "**Thông tin sinh viên** :\n",
        "\n",
        "Hoàng Minh Thanh (18424062)\n",
        "\n",
        "Jupyter notebook (Online) : https://colab.research.google.com/drive/1piW2XzDPXJOg3WBliJAvCGBbQqDXeT8N\n",
        "\n",
        "Thực hiện chạy trên Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YF4ioW3H7k_Y"
      },
      "source": [
        "Cài đặt plugin chạy GPU trên Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lflYvWi_18A7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "7b5a8c60-2615-49cd-ba9b-568896a35ea8"
      },
      "source": [
        "%%bash\n",
        "nvcc --version\n",
        "rm -rf /content/*\n",
        "pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-p7ngbeia\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py): started\n",
            "  Building wheel for NVCCPlugin (setup.py): finished with status 'done'\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=70c439a373062d30599a3f36e709cb93b8deef5d52b6bb7307c8910a7e7358e0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ga32o48y/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-p7ngbeia\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IkSmNRuv794i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d434c878-5378-454b-94bc-6ddf0a18e664"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9qzonG0Jbsy",
        "colab_type": "text"
      },
      "source": [
        "Tải github private repository chứa file in.pnm cần lấy và các file khác\n",
        "\n",
        "Vì đây là private repository nên phải cấu hình clone bằng ssh\n",
        "\n",
        "Tham khảo tại : https://towardsdatascience.com/using-git-with-colab-via-ssh-175e8f3751ec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV19gNnhNzsG",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "bdcf11fb-dfb7-413e-b9ba-071f256173a0"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-450c6555-c080-4937-b658-dfd91d428d06\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-450c6555-c080-4937-b658-dfd91d428d06\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ssh.tar.gz to ssh.tar.gz\n",
            "src  ssh.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWCypga3OKi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "21c4b7fe-edd4-462b-97e5-3e3dad776502"
      },
      "source": [
        "%%bash\n",
        "rm -rf /root/.ssh\n",
        "mkdir /root/.ssh\n",
        "\n",
        "tar xvzf ssh.tar.gz\n",
        "cp ssh-colab/* /root/.ssh && rm -rf ssh-colab && rm -rf ssh.tar.gz\n",
        "chmod 700 /root/.ssh\n",
        "\n",
        "touch /root/.ssh/known_hosts\n",
        "ssh-keyscan github.com >> /root/.ssh/known_hosts\n",
        "chmod 644 /root/.ssh/known_hosts\n",
        "chmod 400 /root/.ssh/id_rsa_thanh"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ssh-colab/\n",
            "ssh-colab/config\n",
            "ssh-colab/id_rsa_thanh\n",
            "ssh-colab/id_rsa_thanh.pub\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "# github.com:22 SSH-2.0-babeld-97a5c183\n",
            "# github.com:22 SSH-2.0-babeld-97a5c183\n",
            "# github.com:22 SSH-2.0-babeld-97a5c183\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APLvDoUmLzpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "rm -rf /content/*\n",
        "git config --global user.email \"hmthanhgm@gmail.com\"\n",
        "git config --global user.name \"hmthanh\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vLs0l8Sebp",
        "colab_type": "text"
      },
      "source": [
        "Di chuyển các file cần thiết ra /content/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7oQ6kRjSir2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87f889d2-6c19-4d07-b73c-5b4cb58fc282"
      },
      "source": [
        "%%bash\n",
        "ssh-agent /bin/bash\n",
        "git clone git@github.com:hmthanh/paralell_programming.git\n",
        "mv /content/paralell_programming/TH_2/* /content/\n",
        "rm -rf /content/paralell_programming/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paralell_programming'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UfRbHHPLOGR0"
      },
      "source": [
        "## Mã nguồn chương trình chính"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jQaNUnaMTQEJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "46714e3a-4240-4c70-cacf-2684f5b58fea"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "#define CHECK(call)                                                            \\\n",
        "{                                                                              \\\n",
        "    const cudaError_t error = call;                                            \\\n",
        "    if (error != cudaSuccess)                                                  \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n",
        "        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n",
        "                cudaGetErrorString(error));                                    \\\n",
        "        exit(EXIT_FAILURE);                                                    \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "struct GpuTimer\n",
        "{\n",
        "    cudaEvent_t start;\n",
        "    cudaEvent_t stop;\n",
        "\n",
        "    GpuTimer()\n",
        "    {\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&stop);\n",
        "    }\n",
        "\n",
        "    ~GpuTimer()\n",
        "    {\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(stop);\n",
        "    }\n",
        "\n",
        "    void Start()\n",
        "    {\n",
        "        cudaEventRecord(start, 0);\n",
        "    }\n",
        "\n",
        "    void Stop()\n",
        "    {\n",
        "        cudaEventRecord(stop, 0);\n",
        "    }\n",
        "\n",
        "    float Elapsed()\n",
        "    {\n",
        "        float elapsed;\n",
        "        cudaEventSynchronize(stop);\n",
        "        cudaEventElapsedTime(&elapsed, start, stop);\n",
        "        return elapsed;\n",
        "    }\n",
        "};\n",
        "\n",
        "__global__ void reduceBlksKernel1(int * in, int n, int * out)\n",
        "{\n",
        "\t// TODO\n",
        "\tint i = blockIdx.x * blockDim.x * 2 + threadIdx.x * 2;\n",
        "    for (int stride = 1; stride < 2 * blockDim.x; stride *= 2)\n",
        "    {\n",
        "        if ((threadIdx.x % stride) == 0)\n",
        "            if (i + stride < n)\n",
        "                in[i] += in[i + stride];\n",
        "\n",
        "        __syncthreads(); // Synchronize within each block\n",
        "    }\n",
        "    \n",
        "    if (threadIdx.x == 0)\n",
        "        out[blockIdx.x] = in[blockIdx.x * blockDim.x * 2];\n",
        "}\n",
        "\n",
        "__global__ void reduceBlksKernel2(int * in, int n, int * out)\n",
        "{\n",
        "\t// TODO\n",
        "\tint numElemsBeforeBlk = blockIdx.x * blockDim.x * 2;\n",
        "    \n",
        "    for (int stride = 1; stride < 2 * blockDim.x; stride *= 2)\n",
        "    {\n",
        "        int i = numElemsBeforeBlk + threadIdx.x * 2 * stride;\n",
        "        //int index = 2 * stride * tid;\n",
        "        if (threadIdx.x < blockDim.x / stride){\n",
        "            if (i + stride < n){\n",
        "                in[i] += in[i + stride];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads(); // Synchronize within each block\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0)\n",
        "        out[blockIdx.x] = in[numElemsBeforeBlk];\n",
        "}\n",
        "\n",
        "__global__ void reduceBlksKernel3(int * in, int n, int * out)\n",
        "{\n",
        "\t// TODO\n",
        "\tint numElemsBeforeBlk = blockIdx.x * blockDim.x * 2;\n",
        "    \n",
        "    for (int stride = blockDim.x; stride > 0; stride >>= 1)\n",
        "    {\n",
        "        int i = numElemsBeforeBlk + threadIdx.x;\n",
        "        if (threadIdx.x < stride)\n",
        "            in[i] += in[i + stride];\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (threadIdx.x == 0)\n",
        "        out[blockIdx.x] = in[numElemsBeforeBlk];\n",
        "}\n",
        "\n",
        "int reduce(int const * in, int n,\n",
        "\t\t\tbool useDevice=false, dim3 blockSize=dim3(1), int kernelType=1)\n",
        "{\n",
        "\n",
        "\tint result = 0; // Init\n",
        "\tif (useDevice == false)\n",
        "\t{\n",
        "\t\tresult = in[0];\n",
        "\t\tfor (int i = 1; i < n; i++)\n",
        "\t\t\tresult += in[i];\n",
        "\t}\n",
        "\telse // Use device\n",
        "\t{\n",
        "\t\t// Allocate device memories\n",
        "\t\tint * d_in, * d_out;\n",
        "\t\tdim3 gridSize((n-1)/(blockSize.x*2) + 1); // TODO: Compute gridSize from n and blockSize\n",
        "\t\tCHECK(cudaMalloc(&d_in, n * sizeof(int)));\n",
        "\t\tCHECK(cudaMalloc(&d_out, gridSize.x * sizeof(int)));\n",
        "\n",
        "\t\t// Copy data to device memory\n",
        "\t\tCHECK(cudaMemcpy(d_in, in, n*sizeof(int), cudaMemcpyHostToDevice));\n",
        "\n",
        "\t\t// Call kernel\n",
        "\t\tGpuTimer timer;\n",
        "\t\ttimer.Start();\n",
        "\t\tif (kernelType == 1)\n",
        "\t\t\treduceBlksKernel1<<<gridSize, blockSize>>>(d_in, n, d_out);\n",
        "\t\telse if (kernelType == 2)\n",
        "\t\t\treduceBlksKernel2<<<gridSize, blockSize>>>(d_in, n, d_out);\n",
        "\t\telse\n",
        "\t\t\treduceBlksKernel3<<<gridSize, blockSize>>>(d_in, n, d_out);\n",
        "\t\ttimer.Stop();\n",
        "\t\tfloat kernelTime = timer.Elapsed();\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\t\tCHECK(cudaGetLastError());\n",
        "\n",
        "\t\t// Copy result from device memory\n",
        "\t\tint * out = (int *)malloc(gridSize.x * sizeof(int));\n",
        "\t\tCHECK(cudaMemcpy(out, d_out, gridSize.x*sizeof(int), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\t\t// Free device memories\n",
        "\t\tCHECK(cudaFree(d_in));\n",
        "\t\tCHECK(cudaFree(d_out));\n",
        "\n",
        "\t\t// Host do the rest of the work\n",
        "\t\ttimer.Start();\n",
        "\t\tresult = out[0];\n",
        "\t\tfor (int i = 1; i < gridSize.x; i++)\n",
        "\t\t{\n",
        "\t\t\tresult += out[i];\n",
        "\t\t}\n",
        "\t\ttimer.Stop();\n",
        "\t\tfloat postKernelTime = timer.Elapsed();\n",
        "\n",
        "\t\t// Free memory\n",
        "\t\tfree(out);\n",
        "\n",
        "\t\t// Print info\n",
        "\t\tprintf(\"\\nKernel %d\\n\", kernelType);\n",
        "\t\tprintf(\"Grid size: %d, block size: %d\\n\", gridSize.x, blockSize.x);\n",
        "\t\tprintf(\"Kernel time = %f ms, post-kernel time = %f ms\\n\", kernelTime, postKernelTime);\n",
        "\t}\n",
        "\n",
        "\treturn result;\n",
        "}\n",
        "\n",
        "void checkCorrectness(int r1, int r2)\n",
        "{\n",
        "\tif (r1 == r2)\n",
        "\t\tprintf(\"CORRECT :)\\n\");\n",
        "\telse\n",
        "\t\tprintf(\"INCORRECT :(\\n\");\n",
        "}\n",
        "\n",
        "void printDeviceInfo()\n",
        "{\n",
        "\tcudaDeviceProp devProv;\n",
        "    CHECK(cudaGetDeviceProperties(&devProv, 0));\n",
        "    printf(\"**********GPU info**********\\n\");\n",
        "    printf(\"Name: %s\\n\", devProv.name);\n",
        "    printf(\"Compute capability: %d.%d\\n\", devProv.major, devProv.minor);\n",
        "    printf(\"Num SMs: %d\\n\", devProv.multiProcessorCount);\n",
        "    printf(\"Max num threads per SM: %d\\n\", devProv.maxThreadsPerMultiProcessor); \n",
        "    printf(\"Max num warps per SM: %d\\n\", devProv.maxThreadsPerMultiProcessor / devProv.warpSize);\n",
        "    printf(\"GMEM: %lu bytes\\n\", devProv.totalGlobalMem);\n",
        "    printf(\"****************************\\n\\n\");\n",
        "\n",
        "}\n",
        "int main(int argc, char ** argv)\n",
        "{\n",
        "\tprintDeviceInfo();\n",
        "\n",
        "\t// Set up input size\n",
        "    int n = (1 << 24) + 1;\n",
        "    printf(\"Input size: %d\\n\", n);\n",
        "\n",
        "    // Set up input data\n",
        "    int * in = (int *) malloc(n * sizeof(int));\n",
        "    for (int i = 0; i < n; i++)\n",
        "    {\n",
        "        // Generate a random integer in [0, 255]\n",
        "        in[i] = (int)(rand() & 0xFF);\n",
        "    }\n",
        "\n",
        "    // Reduce NOT using device\n",
        "    int correctResult = reduce(in, n);\n",
        "\n",
        "    // Reduce using device, kernel1\n",
        "    dim3 blockSize(512); // Default\n",
        "    if (argc == 2)\n",
        "    \tblockSize.x = atoi(argv[1]);\n",
        "    int result1 = reduce(in, n, true, blockSize, 1);\n",
        "    checkCorrectness(result1, correctResult);\n",
        "\n",
        "    // Reduce using device, kernel2\n",
        "    int result2 = reduce(in, n, true, blockSize, 2);\n",
        "    checkCorrectness(result2, correctResult);\n",
        "\n",
        "    // Reduce using device, kernel3\n",
        "    int result3 = reduce(in, n, true, blockSize, 3);\n",
        "    checkCorrectness(result3, correctResult);\n",
        "\n",
        "    // Free memories\n",
        "    free(in);\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.228320 ms, post-kernel time = 0.038944 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.085728 ms, post-kernel time = 0.039072 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 0.542656 ms, post-kernel time = 0.039616 ms\n",
            "CORRECT :)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv-5mKTUrhsH",
        "colab_type": "text"
      },
      "source": [
        "Biên dịch file bt2.cu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx_1ZSEBy8rb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc bt2.cu -o bt2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laRu77wIsKei",
        "colab_type": "text"
      },
      "source": [
        "# Chạy với các kích thước block khác nhau\n",
        "\n",
        "* 1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nje3z2dj73-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "02838b17-e501-4812-c764-6618f1a6b343"
      },
      "source": [
        "!./bt2 1024"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.442144 ms, post-kernel time = 0.031968 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.266368 ms, post-kernel time = 0.020128 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 0.589472 ms, post-kernel time = 0.019424 ms\n",
            "CORRECT :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpDPa3p8tBFh",
        "colab_type": "text"
      },
      "source": [
        "Kiểm tra thông tin phần cứng"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY4mIJFV8gL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7668fe8c-d46d-463f-8ef3-99e3470329ab"
      },
      "source": [
        "%%cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define CHECK(call)                                                            \\\n",
        "{                                                                              \\\n",
        "    const cudaError_t error = call;                                            \\\n",
        "    if (error != cudaSuccess)                                                  \\\n",
        "    {                                                                          \\\n",
        "        fprintf(stderr, \"Error: %s:%d, \", __FILE__, __LINE__);                 \\\n",
        "        fprintf(stderr, \"code: %d, reason: %s\\n\", error,                       \\\n",
        "                cudaGetErrorString(error));                                    \\\n",
        "        exit(EXIT_FAILURE);                                                    \\\n",
        "    }                                                                          \\\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    printf(\"%s Starting...\\n\", argv[0]);\n",
        "\n",
        "    int deviceCount = 0;\n",
        "    cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "    if (deviceCount == 0)\n",
        "    {\n",
        "        printf(\"There are no available device(s) that support CUDA\\n\");\n",
        "    }\n",
        "    else\n",
        "    {\n",
        "        printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
        "    }\n",
        "\n",
        "    int dev = 0, driverVersion = 0, runtimeVersion = 0;\n",
        "    CHECK(cudaSetDevice(dev));\n",
        "    cudaDeviceProp deviceProp;\n",
        "    CHECK(cudaGetDeviceProperties(&deviceProp, dev));\n",
        "    printf(\"Device %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "\n",
        "    cudaDriverGetVersion(&driverVersion);\n",
        "    cudaRuntimeGetVersion(&runtimeVersion);\n",
        "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
        "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
        "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
        "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
        "           deviceProp.major, deviceProp.minor);\n",
        "    printf(\"  Total amount of global memory:                 %.2f GBytes (%llu \"\n",
        "           \"bytes)\\n\", (float)deviceProp.totalGlobalMem / pow(1024.0, 3),\n",
        "           (unsigned long long)deviceProp.totalGlobalMem);\n",
        "    printf(\"  GPU Clock rate:                                %.0f MHz (%0.2f \"\n",
        "           \"GHz)\\n\", deviceProp.clockRate * 1e-3f,\n",
        "           deviceProp.clockRate * 1e-6f);\n",
        "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
        "           deviceProp.memoryClockRate * 1e-3f);\n",
        "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
        "           deviceProp.memoryBusWidth);\n",
        "\n",
        "    if (deviceProp.l2CacheSize)\n",
        "    {\n",
        "        printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
        "               deviceProp.l2CacheSize);\n",
        "    }\n",
        "\n",
        "    printf(\"  Max Texture Dimension Size (x,y,z)             1D=(%d), \"\n",
        "           \"2D=(%d,%d), 3D=(%d,%d,%d)\\n\", deviceProp.maxTexture1D,\n",
        "           deviceProp.maxTexture2D[0], deviceProp.maxTexture2D[1],\n",
        "           deviceProp.maxTexture3D[0], deviceProp.maxTexture3D[1],\n",
        "           deviceProp.maxTexture3D[2]);\n",
        "    printf(\"  Max Layered Texture Size (dim) x layers        1D=(%d) x %d, \"\n",
        "           \"2D=(%d,%d) x %d\\n\", deviceProp.maxTexture1DLayered[0],\n",
        "           deviceProp.maxTexture1DLayered[1], deviceProp.maxTexture2DLayered[0],\n",
        "           deviceProp.maxTexture2DLayered[1],\n",
        "           deviceProp.maxTexture2DLayered[2]);\n",
        "    printf(\"  Total amount of constant memory:               %lu bytes\\n\",\n",
        "           deviceProp.totalConstMem);\n",
        "    printf(\"  Total amount of shared memory per block:       %lu bytes\\n\",\n",
        "           deviceProp.sharedMemPerBlock);\n",
        "    printf(\"  Total number of registers available per block: %d\\n\",\n",
        "           deviceProp.regsPerBlock);\n",
        "    printf(\"  Warp size:                                     %d\\n\",\n",
        "           deviceProp.warpSize);\n",
        "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
        "           deviceProp.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
        "           deviceProp.maxThreadsPerBlock);\n",
        "    printf(\"  Maximum sizes of each dimension of a block:    %d x %d x %d\\n\",\n",
        "           deviceProp.maxThreadsDim[0],\n",
        "           deviceProp.maxThreadsDim[1],\n",
        "           deviceProp.maxThreadsDim[2]);\n",
        "    printf(\"  Maximum sizes of each dimension of a grid:     %d x %d x %d\\n\",\n",
        "           deviceProp.maxGridSize[0],\n",
        "           deviceProp.maxGridSize[1],\n",
        "           deviceProp.maxGridSize[2]);\n",
        "    printf(\"  Maximum memory pitch:                          %lu bytes\\n\",\n",
        "           deviceProp.memPitch);\n",
        "\n",
        "    exit(EXIT_SUCCESS);\n",
        "}\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpnj7cq7n6/fd0cc78d-12d4-4535-bb6e-d86205b99720.out Starting...\n",
            "Detected 1 CUDA Capable device(s)\n",
            "Device 0: \"Tesla P100-PCIE-16GB\"\n",
            "  CUDA Driver Version / Runtime Version          10.1 / 10.1\n",
            "  CUDA Capability Major/Minor version number:    6.0\n",
            "  Total amount of global memory:                 15.90 GBytes (17071734784 bytes)\n",
            "  GPU Clock rate:                                1329 MHz (1.33 GHz)\n",
            "  Memory Clock rate:                             715 Mhz\n",
            "  Memory Bus Width:                              4096-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Max Texture Dimension Size (x,y,z)             1D=(131072), 2D=(131072,65536), 3D=(16384,16384,16384)\n",
            "  Max Layered Texture Size (dim) x layers        1D=(32768) x 2048, 2D=(32768,32768) x 2048\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Maximum sizes of each dimension of a block:    1024 x 1024 x 64\n",
            "  Maximum sizes of each dimension of a grid:     2147483647 x 65535 x 65535\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O4FljrsD8WkP"
      },
      "source": [
        "* 1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wL0pPOA8itB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "74c67f42-f354-4c0f-ce26-7265c2aa6e10"
      },
      "source": [
        "!./bt2 1024"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.440032 ms, post-kernel time = 0.020352 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 1.264512 ms, post-kernel time = 0.018720 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 8193, block size: 1024\n",
            "Kernel time = 0.584640 ms, post-kernel time = 0.018976 ms\n",
            "CORRECT :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rWWFlO-P8XAY"
      },
      "source": [
        "* 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIC-JWoe8leh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "34f319d0-8812-453a-b9fc-5c21f8e6c40c"
      },
      "source": [
        "!./bt2 512"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.225440 ms, post-kernel time = 0.041568 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 1.101728 ms, post-kernel time = 0.038592 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 16385, block size: 512\n",
            "Kernel time = 0.551616 ms, post-kernel time = 0.041472 ms\n",
            "CORRECT :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K1UDv2Cx8XC_"
      },
      "source": [
        "* 256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBoEbqYO7-A2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "6245f0cb-78b8-409a-b194-f73da791b575"
      },
      "source": [
        "!./bt2 256"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 1.084576 ms, post-kernel time = 0.083904 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 0.976128 ms, post-kernel time = 0.165664 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 32769, block size: 256\n",
            "Kernel time = 0.505920 ms, post-kernel time = 0.096064 ms\n",
            "CORRECT :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgo3EQ-A8ugA",
        "colab_type": "text"
      },
      "source": [
        "* 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7zdaU5TwKsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "e804fae7-8d41-48a1-9634-a8da22838640"
      },
      "source": [
        "!./bt2 128"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********GPU info**********\n",
            "Name: Tesla P100-PCIE-16GB\n",
            "Compute capability: 6.0\n",
            "Num SMs: 56\n",
            "Max num threads per SM: 2048\n",
            "Max num warps per SM: 64\n",
            "GMEM: 17071734784 bytes\n",
            "****************************\n",
            "\n",
            "Input size: 16777217\n",
            "\n",
            "Kernel 1\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 0.977472 ms, post-kernel time = 0.165504 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 2\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 0.886144 ms, post-kernel time = 0.166240 ms\n",
            "CORRECT :)\n",
            "\n",
            "Kernel 3\n",
            "Grid size: 65537, block size: 128\n",
            "Kernel time = 0.455392 ms, post-kernel time = 0.173696 ms\n",
            "CORRECT :)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAoISYbI80Zh",
        "colab_type": "text"
      },
      "source": [
        "### Giải thích :\n",
        "\n",
        "Xảy ra lỗi như vậy là do các thread của mỗi block được phân vào các wrap (32 thread) khác nhau.\n",
        "Trong GPU thì mỗi thread trong một wrap thực thi bất đồng bộ sử dụng chung một thanh nhớ register (bộ nhớ tạm trên cùng một wrap) nên có thể với kích thước filter lớn hơn 32 thì quá trình tính toán song song sẽ dấn đến một thread đã xử lý xong và cập nhật kết quả mới vào bộ nhớ toàn cục nhưng thread khác đang xử lý sẽ lấy kết quả pixels từ bộ nhớ tạm trên thanh ghi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6Hdg_7C8ohX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0f0e0ade-0d26-4225-82f5-e7261a583c61"
      },
      "source": [
        "!nvcc bt2.cu -o btdemo1 -Xptxas=\"-v\"\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel3PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel3PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 344 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel2PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel2PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 13 registers, 344 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel1PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel1PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 14 registers, 344 bytes cmem[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AxS-Wd2XKHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6270e82c-8dbd-408f-9b6d-fa215bec3c18"
      },
      "source": [
        "!nvcc bt2.cu -o btdemo2 --ptxas-options=-v"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel3PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel3PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 344 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel2PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel2PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 13 registers, 344 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z17reduceBlksKernel1PiiS_' for 'sm_30'\n",
            "ptxas info    : Function properties for _Z17reduceBlksKernel1PiiS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 14 registers, 344 bytes cmem[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6cRU808zb-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDwEdkmj1pM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}